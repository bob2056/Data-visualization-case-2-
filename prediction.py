# -*- coding: utf-8 -*-
"""Untitled9.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NOrbx9e5WDWbMD2I_CO8593hgaWVMfAY
"""

# Preprocessing script for Chicago_Crimes_2012_to_2017.csv
# Requires: pandas, numpy
import pandas as pd
import numpy as np

INPUT = "/content/Chicago_Crimes_2012_to_2017.csv"   # change if necessary
OUTPUT = "/mnt/data/Chicago_Crimes_2012_2017_cleaned.csv"

# 0. Read (use chunking if memory-limited)
df = pd.read_csv(INPUT, low_memory=False)

# 1. Remove redundant columns
for c in ['Unnamed: 0', 'Unnamed:0', 'Unnamed: 0.1']:
    if c in df.columns:
        df = df.drop(columns=[c])

# 2. Parse dates and extract temporal features (fast path using known format)
if 'Date' in df.columns:
    fmt = "%m/%d/%Y %I:%M:%S %p"
    try:
        df['IncidentDate'] = pd.to_datetime(df['Date'], format=fmt, errors='coerce')
    except Exception:
        df['IncidentDate'] = pd.to_datetime(df['Date'], errors='coerce', infer_datetime_format=True)
    df['Year'] = df['IncidentDate'].dt.year
    df['Month'] = df['IncidentDate'].dt.month
    df['Day'] = df['IncidentDate'].dt.day
    df['Hour'] = df['IncidentDate'].dt.hour
    df['Weekday'] = df['IncidentDate'].dt.day_name()
else:
    df['IncidentDate'] = pd.NaT

# 3. Standardize textual fields
text_cols = ['Primary Type', 'Description', 'Location Description', 'Block']
for c in text_cols:
    if c in df.columns:
        df[c] = df[c].astype(str).str.strip()
        if c != 'Block':
            df[c] = df[c].str.upper()
        df[c].replace({'nan': None, 'None': None}, inplace=True)

# 4. Convert boolean flags to ints
if 'Arrest' in df.columns:
    df['Arrest'] = df['Arrest'].replace({True:1,False:0,'true':1,'TRUE':1,'FALSE':0,'false':0})
    df['Arrest'] = pd.to_numeric(df['Arrest'], errors='coerce').fillna(0).astype(int)
if 'Domestic' in df.columns:
    df['Domestic'] = df['Domestic'].replace({True:1,False:0,'true':1,'TRUE':1,'FALSE':0,'false':0})
    df['Domestic'] = pd.to_numeric(df['Domestic'], errors='coerce').fillna(0).astype(int)

# 5. Geospatial: ensure numeric, impute missing lat/lon by Community Area median -> Ward -> global
lat_col = next((c for c in df.columns if c.lower().startswith('lat')), None)
lon_col = next((c for c in df.columns if c.lower().startswith('lon')), None)

group_col = next((c for c in df.columns if c.lower().startswith('community')), None)
ward_col = next((c for c in df.columns if c.lower()=='ward'), None)

if lat_col and lon_col:
    df[lat_col] = pd.to_numeric(df[lat_col], errors='coerce')
    df[lon_col] = pd.to_numeric(df[lon_col], errors='coerce')
    global_lat = df[lat_col].median(skipna=True)
    global_lon = df[lon_col].median(skipna=True)
    if group_col:
        med = df.groupby(group_col)[[lat_col, lon_col]].median().rename(columns={lat_col:'lat_med', lon_col:'lon_med'})
        med = med.reset_index()
        df = df.merge(med, how='left', left_on=group_col, right_on=group_col)
    elif ward_col:
        med = df.groupby(ward_col)[[lat_col, lon_col]].median().rename(columns={lat_col:'lat_med', lon_col:'lon_med'}).reset_index()
        df = df.merge(med, how='left', left_on=ward_col, right_on=ward_col)
    else:
        df['lat_med'] = global_lat
        df['lon_med'] = global_lon
    df[lat_col] = df[lat_col].fillna(df['lat_med']).fillna(global_lat)
    df[lon_col] = df[lon_col].fillna(df['lon_med']).fillna(global_lon)
    df = df.drop(columns=[c for c in ['lat_med','lon_med'] if c in df.columns])

# 6. Handle missing categories and duplicates
if 'Primary Type' in df.columns:
    df['Primary Type'] = df['Primary Type'].fillna('UNKNOWN')
if 'Location Description' in df.columns:
    df['Location Description'] = df['Location Description'].fillna('UNKNOWN')

# Drop duplicates by ID or Case Number if present
if 'ID' in df.columns:
    df = df.drop_duplicates(subset=['ID'])
elif 'Case Number' in df.columns:
    df = df.drop_duplicates(subset=['Case Number'])

# 7. Final - drop rows missing essential fields (if needed)
essential = []
if lat_col and lon_col:
    essential += [lat_col, lon_col]
if 'Primary Type' in df.columns:
    essential += ['Primary Type']
if essential:
    df = df.dropna(subset=essential)

# Save cleaned file
df.to_csv(OUTPUT, index=False)
print("Saved cleaned file to:", OUTPUT)
print("Original shape:", df.shape)

import os

# Create the directory if it doesn't exist
output_dir = "/mnt/data"
if not os.path.exists(output_dir):
    os.makedirs(output_dir)
    print(f"Created directory: {output_dir}")
else:
    print(f"Directory already exists: {output_dir}")